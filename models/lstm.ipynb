{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "# 定义 PyTorch 数据集\n",
    "class HardDriveDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)  # 转为张量\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)  # 转为张量\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def normalize_smart_data(data, feature_indices):\n",
    "    \"\"\"\n",
    "    对 SMART 数据进行归一化，每个特征独立归一化。\n",
    "    :param data: ndarray，形状为 (num_records, max_days, features_per_day)\n",
    "    :param feature_indices: list，每个特征的索引（时间戳不参与归一化）\n",
    "    :return: 归一化后的数据\n",
    "    \"\"\"\n",
    "    normalized_data = data.copy()\n",
    "    for idx in feature_indices:\n",
    "        feature_values = data[:, :, idx]\n",
    "        min_val = np.min(feature_values)\n",
    "        max_val = np.max(feature_values)\n",
    "        if max_val > min_val:  # 避免除以零\n",
    "            normalized_data[:, :, idx] = (feature_values - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            normalized_data[:, :, idx] = 0  # 如果特征是常量，则设为0\n",
    "    return normalized_data\n",
    "\n",
    "def prepare_data(data_list, max_days=10, features_per_day=3):\n",
    "    X, y = [], []\n",
    "    for record in data_list:\n",
    "        if 'data' not in record or 'label' not in record:\n",
    "            continue\n",
    "        data = record['data']\n",
    "        label = record['label']\n",
    "\n",
    "        split_data = \" \".join(map(str, data)).split('<SEP>')\n",
    "        record_data = []\n",
    "\n",
    "        for day in split_data:\n",
    "            day_data = day.split()\n",
    "            if len(day_data) > 1:\n",
    "                # 跳过第一个时间戳字段，处理 SMART 数据\n",
    "                smart_data = list(map(float, day_data[1:]))\n",
    "                record_data.append(smart_data)\n",
    "            else:\n",
    "                record_data.append([0] * features_per_day)  # 填充空天\n",
    "\n",
    "        # 填充到 max_days\n",
    "        while len(record_data) < max_days:\n",
    "            record_data.append([0] * features_per_day)\n",
    "        record_data = record_data[:max_days]\n",
    "\n",
    "        X.append(record_data)\n",
    "        y.append(label)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # 填充 NaN 为 0\n",
    "    X = np.nan_to_num(X, nan=0)\n",
    "\n",
    "    # 归一化 SMART 数据\n",
    "    feature_indices = list(range(1, features_per_day))  # 跳过第一个时间戳字段\n",
    "    X = normalize_smart_data(X, feature_indices)\n",
    "\n",
    "    return X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = x * mask  # 忽略无效输入\n",
    "        print(f\"输入最大值: {torch.max(x)}, 最小值: {torch.min(x)}, 均值: {torch.mean(x)}\")  # 检查输入\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        print(f'hidden: \\n{hidden}')\n",
    "        out = self.fc(hidden[-1])\n",
    "        return self.sigmoid(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate_lstm(X, y, input_dim, hidden_dim=64, num_layers=2, batch_size=64, epochs=10, lr=0.001):\n",
    "    # 划分数据集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # 转换为 DataLoader\n",
    "    train_dataset = HardDriveDataset(X_train, y_train)\n",
    "    test_dataset = HardDriveDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = LSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=1, num_layers=num_layers)\n",
    "    criterion = nn.BCELoss()  # 或 nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # 记录损失\n",
    "    train_losses = []\n",
    "    \n",
    "    # 训练模型\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # mask_batch = (X_batch != 0).float()  # 构建掩码\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            # print(f'X_batch: {X_batch}')\n",
    "            print(f'output: {outputs}')\n",
    "            print(f'y_batch: {y_batch}')\n",
    "            break\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # 显示损失曲线\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, epochs + 1), train_losses, marker='o', label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    # 模型评估\n",
    "    model.eval()\n",
    "    y_pred, y_true, y_proba = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            preds = (outputs > 0.5).float()\n",
    "            y_pred.extend(preds.tolist())\n",
    "            y_true.extend(y_batch.tolist())\n",
    "            y_proba.extend(outputs.tolist())\n",
    "    \n",
    "    # 计算性能指标\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    print(f\"TPR: {tpr:.4f}, FPR: {fpr:.4f}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据加载函数\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "def load_data_from_folder(folder_path):\n",
    "    all_data = []\n",
    "    for file in tqdm(os.listdir(folder_path)):\n",
    "        if file.endswith('.pkl'):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                all_data.extend(data)  # 每个文件有 10000 条记录，合并到一起\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_folder):\n",
    "    # 加载数据\n",
    "    data = load_data_from_folder(data_folder)\n",
    "    print(f\"Total records loaded: {len(data)}\")\n",
    "    \n",
    "    # 数据预处理\n",
    "    max_days = 10\n",
    "    features_per_day = 3  # 根据 SMART 数据的数量调整\n",
    "    X, y = prepare_data(data, max_days=max_days, features_per_day=features_per_day)\n",
    "    print(f\"Data shape: {X.shape}, Labels shape: {y.shape}\")\n",
    "    \n",
    "    # 模型训练与评估\n",
    "    input_dim = features_per_day\n",
    "    model = train_and_evaluate_lstm(X, y, input_dim=input_dim)\n",
    "    print(\"Model training and evaluation complete.\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  8.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records loaded: 30002\n",
      "Data shape: (30002, 10, 12), Labels shape: (30002,)\n",
      "输入最大值: 4294967296.0, 最小值: 0.0, 均值: 54858688.0\n",
      "hidden: \n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], grad_fn=<StackBackward0>)\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "y_batch: tensor([1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "Epoch 1/10, Loss: 0.0000\n",
      "输入最大值: 4294967296.0, 最小值: 0.0, 均值: 74319472.0\n",
      "hidden: \n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], grad_fn=<StackBackward0>)\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "y_batch: tensor([1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 1., 0., 0., 1., 0., 0., 0., 0.])\n",
      "Epoch 2/10, Loss: 0.0000\n",
      "输入最大值: 4294967296.0, 最小值: 0.0, 均值: 88149432.0\n",
      "hidden: \n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], grad_fn=<StackBackward0>)\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "y_batch: tensor([1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 0., 1., 0., 0., 0., 0.])\n",
      "Epoch 3/10, Loss: 0.0000\n",
      "输入最大值: 4294967296.0, 最小值: 0.0, 均值: 24516030.0\n",
      "hidden: \n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], grad_fn=<StackBackward0>)\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "y_batch: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Epoch 4/10, Loss: 0.0000\n",
      "输入最大值: 4294967296.0, 最小值: 0.0, 均值: 62597060.0\n",
      "hidden: \n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], grad_fn=<StackBackward0>)\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "y_batch: tensor([0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 1., 0., 0.])\n",
      "Epoch 5/10, Loss: 0.0000\n",
      "输入最大值: 4294967296.0, 最小值: 0.0, 均值: 74908800.0\n",
      "hidden: \n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], grad_fn=<StackBackward0>)\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "y_batch: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 1., 0., 1., 0., 1., 0., 0., 1.])\n",
      "Epoch 6/10, Loss: 0.0000\n",
      "输入最大值: 4294967296.0, 最小值: 0.0, 均值: 55850668.0\n",
      "hidden: \n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], grad_fn=<StackBackward0>)\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "y_batch: tensor([1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 1., 1.])\n",
      "Epoch 7/10, Loss: 0.0000\n",
      "输入最大值: 4294967296.0, 最小值: 0.0, 均值: 58428388.0\n",
      "hidden: \n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], grad_fn=<StackBackward0>)\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "y_batch: tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 0., 0., 1.])\n",
      "Epoch 8/10, Loss: 0.0000\n",
      "输入最大值: 4294967296.0, 最小值: 0.0, 均值: 66424592.0\n",
      "hidden: \n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], grad_fn=<StackBackward0>)\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "y_batch: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 1., 0., 0., 0., 1.])\n",
      "Epoch 9/10, Loss: 0.0000\n",
      "输入最大值: 4294967296.0, 最小值: 0.0, 均值: 73314408.0\n",
      "hidden: \n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], grad_fn=<StackBackward0>)\n",
      "output: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "y_batch: tensor([0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        0., 1., 1., 0., 0., 0., 0., 1., 1., 1.])\n",
      "Epoch 10/10, Loss: 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/raid5/sum/card/storage/StreamDFP/dataset/train\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# 替换为实际数据路径\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m lstm_model \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_folder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(data_folder)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 模型训练与评估\u001b[39;00m\n\u001b[1;32m     13\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m features_per_day\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel training and evaluation complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "Cell \u001b[0;32mIn[9], line 60\u001b[0m, in \u001b[0;36mtrain_and_evaluate_lstm\u001b[0;34m(X, y, input_dim, hidden_dim, num_layers, batch_size, epochs, lr)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m---> 60\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     61\u001b[0m         preds \u001b[38;5;241m=\u001b[39m (outputs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     62\u001b[0m         y_pred\u001b[38;5;241m.\u001b[39mextend(preds\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[0;32m~/anaconda3/envs/sent_train/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/sent_train/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'mask'"
     ]
    }
   ],
   "source": [
    "data_folder = \"/mnt/raid5/sum/card/storage/StreamDFP/dataset/train\"  # 替换为实际数据路径\n",
    "lstm_model = main(data_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sent_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
