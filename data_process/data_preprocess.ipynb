{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-03 03:00:26 2018-02-11 03:00:26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:12<00:00,  3.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# 未来优化：可以通过调整存储的csv窗口大小来加速，但是需要考虑window\n",
    "\n",
    "# Parameters\n",
    "# smart_data_base_folder = '/mnt/raid5/sum/card/storage/StreamDFP/dataset/SMART'\n",
    "failure_file_path = '/mnt/raid5/sum/card/storage/StreamDFP/dataset/ssd_failure_label.csv'\n",
    "smart_data_base_folder = '/mnt/raid5/sum/card/storage/StreamDFP/dataset/processed_smart_data'\n",
    "\n",
    "lookback_days = 20  # 前30天的数据\n",
    "window_size = 10  # 每个窗口大小为10天\n",
    "step_size = 5  # 每次滑动1天\n",
    "cache_days = 20\n",
    "\n",
    "separator = '<SEP>'\n",
    "\n",
    "# 分批保存文件的基础名\n",
    "output_file_prefix = \"processed_data_part\"\n",
    "\n",
    "\n",
    "failure_file_path = '/mnt/raid5/sum/card/storage/StreamDFP/dataset/ssd_failure_label.csv'  # 设置为failure数据的路径\n",
    "\n",
    "# 加载失败数据\n",
    "failure_data = pd.read_csv(failure_file_path)\n",
    "failure_data['failure_time'] = pd.to_datetime(failure_data['failure_time'])\n",
    "\n",
    "# 首先对failure_data升序排序\n",
    "failure_data = failure_data.sort_values(by='failure_time', ascending=True)\n",
    "\n",
    "# 因为如果从20180101的错误开始计算，由于没有2018年之前的数据，所以这些输入会不匹配。因此我们直接从2018年1月22日的数据开始计算。\n",
    "filter_failure_data = pd.to_datetime('2018-01-22')\n",
    "failure_data = failure_data[failure_data['failure_time'] >= filter_failure_data]\n",
    "\n",
    "end_date = failure_data.iloc[0].failure_time + pd.Timedelta(days=cache_days)\n",
    "\n",
    "start_date = max(end_date - timedelta(days=lookback_days +  cache_days - 1), pd.to_datetime('2018-01-01'))\n",
    "print(start_date, end_date)\n",
    "# 获取日期范围\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "# 读取并合并 CSV 文件\n",
    "all_data = []\n",
    "\n",
    "for date in tqdm(date_range):\n",
    "    # 格式化日期为 YYYYMMDD\n",
    "    date_str = date.strftime('%Y%m%d')\n",
    "    file_name = f\"{date_str}_processed.csv\"\n",
    "    \n",
    "    try:\n",
    "        # 读取 CSV 文件，并附加到 all_data 列表\n",
    "        df = pd.read_csv(os.path.join(smart_data_base_folder, file_name))\n",
    "        # df['date'] = date_str  # 添加文件日期作为一列\n",
    "        all_data.append(df)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"文件 {file_name} 未找到，跳过该文件\")\n",
    "\n",
    "range_csv = pd.concat(all_data, ignore_index=True)\n",
    "type(range_csv['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_csv['ds'] = pd.to_datetime(range_csv['ds'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 40/16241 [00:42<4:44:44,  1.05s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 49\u001b[0m\n\u001b[1;32m     43\u001b[0m end_time \u001b[38;5;241m=\u001b[39m failure_time\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# 在 range_csv 中筛选出 disk_id 和 model 与当前 failure 相同的数据\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# matching_data = range_csv[(range_csv['disk_id'] == disk_id) & (range_csv['model'] == model)]\u001b[39;00m\n\u001b[1;32m     47\u001b[0m matching_data \u001b[38;5;241m=\u001b[39m range_csv[\n\u001b[1;32m     48\u001b[0m     (range_csv[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisk_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m disk_id) \u001b[38;5;241m&\u001b[39m\n\u001b[0;32m---> 49\u001b[0m     (\u001b[43mrange_csv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m     50\u001b[0m     (range_csv[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m start_time) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m     51\u001b[0m     (range_csv[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m end_time)\n\u001b[1;32m     52\u001b[0m ]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# print(matching_data)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(matching_data) \u001b[38;5;241m-\u001b[39m window_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, step_size):\n",
      "File \u001b[0;32m~/anaconda3/envs/sent_train/lib/python3.8/site-packages/pandas/core/ops/common.py:81\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     79\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/sent_train/lib/python3.8/site-packages/pandas/core/arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/sent_train/lib/python3.8/site-packages/pandas/core/series.py:6096\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6093\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   6095\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 6096\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/sent_train/lib/python3.8/site-packages/pandas/core/ops/array_ops.py:293\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_object_dtype(lvalues\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    296\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/sent_train/lib/python3.8/site-packages/pandas/core/ops/array_ops.py:82\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m     80\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mvec_compare(x\u001b[38;5;241m.\u001b[39mravel(), y\u001b[38;5;241m.\u001b[39mravel(), op)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlibops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_data = []\n",
    "save_count = 0\n",
    "output_folder = '/mnt/raid5/sum/card/storage/StreamDFP/dataset/train'\n",
    "for failure in tqdm(failure_data.itertuples(), total=len(failure_data)):\n",
    "    # 如果当前时间 - lookback_days > start_date, 则计算超过的时间\n",
    "    \n",
    "    if failure.failure_time > end_date + timedelta(days=1):\n",
    "        # days = (failure.failure_time - end_date).days\n",
    "        days = min((failure.failure_time - end_date).days, (pd.to_datetime('2019-12-31') - end_date).days)\n",
    "\n",
    "        print(f'Failure time: {failure.failure_time}, days: {days}')\n",
    "        \n",
    "        # 生成要删除的日期范围\n",
    "        delete_dates = [(start_date + timedelta(days=i)).strftime('%Y%m%d') for i in range(days)]\n",
    "\n",
    "        # 删除 range_csv 中指定日期的数据\n",
    "        # range_csv = range_csv[~range_csv['date'].isin(delete_dates)]\n",
    "        \n",
    "        # 加载并加入 end_date+1 和 end_date+2 的数据\n",
    "        for i in range(1, days + 1):\n",
    "            new_date = end_date + timedelta(days=i)\n",
    "            new_date_str = new_date.strftime('%Y%m%d')\n",
    "            file_name = f\"{new_date_str}_processed.csv\"\n",
    "            \n",
    "            try:\n",
    "                new_data = pd.read_csv(os.path.join(smart_data_base_folder, file_name))\n",
    "                # new_data['date'] = new_date_str\n",
    "                range_csv = pd.concat([range_csv, new_data], ignore_index=True)\n",
    "                range_csv['ds'] = pd.to_datetime(range_csv['ds'], format='%Y-%m-%d')\n",
    "            except FileNotFoundError:\n",
    "                print(f\"文件 {file_name} 未找到，跳过该文件\")\n",
    "\n",
    "        # 更新 start_date 和 end_date\n",
    "        start_date = start_date + timedelta(days=days)\n",
    "        end_date = end_date + timedelta(days=days)\n",
    "\n",
    "    # 从 failure 中获取 disk_id 和 model\n",
    "    disk_id = failure.disk_id\n",
    "    model = failure.model\n",
    "    failure_time = failure.failure_time\n",
    "    \n",
    "    start_time = failure_time - timedelta(days=lookback_days)\n",
    "    end_time = failure_time\n",
    "\n",
    "    # 在 range_csv 中筛选出 disk_id 和 model 与当前 failure 相同的数据\n",
    "    # matching_data = range_csv[(range_csv['disk_id'] == disk_id) & (range_csv['model'] == model)]\n",
    "    matching_data = range_csv[\n",
    "        (range_csv['disk_id'] == disk_id) &\n",
    "        (range_csv['model'] == model) &\n",
    "        (range_csv['ds'] >= start_time) &\n",
    "        (range_csv['ds'] <= end_time)\n",
    "    ]\n",
    "    # print(matching_data)\n",
    "    for start_idx in range(0, len(matching_data) - window_size + 1, step_size):\n",
    "        end_idx = start_idx + window_size\n",
    "        window = matching_data.iloc[start_idx:end_idx]  # 当前窗口数据\n",
    "        \n",
    "        # 序列化每个窗口的数据\n",
    "        serialized_data = []\n",
    "        for _, row in window.iterrows():\n",
    "            # 将单天数据转换为 [ds, r_1, r_9, ...] 格式\n",
    "            day_data = [row['ds']] + row.filter(regex='^r_').tolist()\n",
    "            serialized_data.extend(day_data)  # 将单天数据加入序列\n",
    "            serialized_data.append(separator)  # 插入分隔符\n",
    "        \n",
    "        serialized_data = serialized_data[:-1]  # 移除最后一个多余的分隔符\n",
    "        \n",
    "        total_data.append({'disk_id': disk_id, 'model': model, 'data': serialized_data, 'label': window.iloc[-1].label})  # 添加失败数据\n",
    "    \n",
    "    if len(total_data) >= 10000:\n",
    "        # 保存数据\n",
    "        with open(os.path.join(output_folder, f\"{output_file_prefix}_{save_count}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(total_data, f)\n",
    "        print(f\"保存 {len(total_data)} 条数据到 {output_folder}/{output_file_prefix}_{save_count}.pkl\")\n",
    "        save_count += 1\n",
    "        total_data = []\n",
    "    # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sent_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
